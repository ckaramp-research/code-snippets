{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset and regrid CMIP6 data from ESGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on CoLab\")\n",
    "    !pip install zarr==2.18 cftime intake-esgf rooki\n",
    "else:\n",
    "    print(\"Not running on CoLab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showcases how to use `intake_esfg` to search and filter the CMIP6 collection, and how to use `rooki` to subset and regrid the data on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import intake_esgf\n",
    "\n",
    "# Run this on the DKRZ node in Germany, using the ESGF1 index node at LLNL\n",
    "os.environ[\"ROOK_URL\"] = \"http://rook.dkrz.de/wps\"\n",
    "# data download directory\n",
    "import os\n",
    "\n",
    "os.environ[\"ROOKI_OUTPUT_DIR\"] = os.path.join(os.getcwd(), \"rookie_output\")\n",
    "\n",
    "intake_esgf.conf.set(\n",
    "    indices={\"anl-dev\": False, \"ornl-dev\": False, \"esgf-node.llnl.gov\": True}\n",
    ")\n",
    "\n",
    "import xarray as xr\n",
    "from intake_esgf import ESGFCatalog\n",
    "from rooki import operators as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve subset of CMIP6 data\n",
    "\n",
    "The CMIP6 dataset is identified by a dataset-id. Using intake-esgf we can query the ESGF database for the variables and models we are interested in. For this demo we are interested in the tos (sea surface temperature) variable for the historical runs. Also, for sake of simplicity we will only query a subset of the models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ESGFCatalog()\n",
    "cat.search(\n",
    "    experiment_id=[\"historical\"],\n",
    "    variable_id=[\"tos\"],\n",
    "    table_id=[\"Omon\"],\n",
    "    project=[\"CMIP6\"],\n",
    "    grid_label=[\"gn\"],\n",
    "    source_id=[\n",
    "        \"CESM2-FV2\",\n",
    "        \"CESM2-WACCM-FV2\",\n",
    "        \"FGOALS-f3-L\",\n",
    "        \"MIROC-ES2L\",\n",
    "    ],\n",
    ")\n",
    "cat.remove_ensembles()  # we only want to work with the parent datasets\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the catalog has been queried, we have to do some manipulation in pandas to keep only the dataset_id. This has to be done because the same data has multiple locations online, and these get appended at the end of the dataset_id. Rookie only accepts the dataset_id without the online location, so we get rid of it in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.df.id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_ds_id(ds):\n",
    "    return ds[0].split(\"|\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These paths are what we are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = cat.df.id.apply(keep_ds_id).to_list()\n",
    "collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset and regrid\n",
    "We define a function that will do the subset and regridding for us for each of the dataset_ids we have. The function will take the dataset_id as input and then use Rookie functions to select 100 years of data for the tos variable in the Pacific Ocean region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the operations, you can go to [rook's documentation](https://rook-wps.readthedocs.io/en/latest/processes.html#).\n",
    "\n",
    "For regridding, refer to this [source code](https://github.com/roocs/rook/blob/main/src/rook/processes/wps_regrid.py)\n",
    "\n",
    "\n",
    "**Note:** Some dataset requests might fail when querying more than 25 years of data (might be size related, needs more testing). So it would be safer to keep the request below that threshold and implement a loop to retrieve more data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pacific_ocean(dataset_id):\n",
    "    wf = ops.Regrid(\n",
    "        ops.Subset(\n",
    "            ops.Input(\"tos\", [dataset_id]),\n",
    "            time=\"1850-01-01/1875-01-31\",\n",
    "            area=\"100,-20,280,20\",\n",
    "        ),\n",
    "        method=\"bilinear\",\n",
    "        grid=\"1deg\",\n",
    "    )\n",
    "    resp = wf.orchestrate()\n",
    "    if resp.ok:\n",
    "        print(f\"{resp.size_in_mb=}\")\n",
    "        ds = resp.datasets()[0]\n",
    "    else:\n",
    "        raise ValueError(resp)\n",
    "        # ds = xr.Dataset()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_data = {dset: get_pacific_ocean(dset) for dset in collections}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be downloaded to a temp folder in our local machine. We can then explore the data using xarray or any other tool of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting data with vertical levels\n",
    "\n",
    "This process should be similar to what we have already explored before. The general steps for requesting data are as follows:\n",
    "\n",
    "1. Use `intake_esgf` to search for the dataset of interest.\n",
    "2. Filter the results to get the dataset_ids we want.\n",
    "3. Use `rooki` to subset and regrid the data.\n",
    "4. Download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ESGFCatalog()\n",
    "cat.search(\n",
    "    experiment_id=[\"historical\"],\n",
    "    variable_id=[\"thetao\"],\n",
    "    table_id=[\"Omon\"],\n",
    "    project=[\"CMIP6\"],\n",
    "    grid_label=[\"gn\"],\n",
    "    source_id=[\n",
    "        \"CESM2-FV2\",\n",
    "        \"CESM2-WACCM-FV2\",\n",
    "        \"FGOALS-f3-L\",\n",
    "        \"MIROC-ES2L\",\n",
    "    ],\n",
    ")\n",
    "cat.remove_ensembles()\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = cat.df.id.apply(keep_ds_id).to_list()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pacific_ocean(dataset_id):\n",
    "    wf = ops.Regrid(\n",
    "        ops.Subset(\n",
    "            ops.Input(\"thetao\", [dataset_id]),\n",
    "            time=\"1850-01-01/1851-01-31\",\n",
    "            area=\"100,-10,280,10\",\n",
    "            level=\"0/50\",\n",
    "        ),\n",
    "        method=\"billinear\",\n",
    "        grid=\"2pt5deg\",\n",
    "    )\n",
    "    resp = wf.orchestrate()\n",
    "    if resp.ok:\n",
    "        print(f\"{resp.size_in_mb=}\")\n",
    "        ds = resp.datasets()[0]\n",
    "    else:\n",
    "        ds = xr.Dataset()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take some time to complete depending on the region and time selection. Another convenient method to load this data is to use the google cloud storage bucket, which can be find [here](https://github.com/ckaramp-research/code-snippets/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_data = {dset: get_pacific_ocean(dset) for dset in collections}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enso2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
